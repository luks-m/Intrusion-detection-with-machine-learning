50 epochs

2 neurons : loss: 0.5128 - accuracy: 0.7308 - val_loss: 0.5128 - val_accuracy: 0.7300
3 neurons : loss: 0.3574 - accuracy: 0.8359 - val_loss: 0.3396 - val_accuracy: 0.8466
4 neurons : loss: 0.3637 - accuracy: 0.8302 - val_loss: 0.3592 - val_accuracy: 0.8376
5 neurons : loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000 --> overfitting

25 epochs : 

2 neurons : loss: 0.5703 - accuracy: 0.7208 - val_loss: 0.5687 - val_accuracy: 0.7263
3 neurons : loss: 0.5337 - accuracy: 0.7337 - val_loss: 0.5325 - val_accuracy: 0.7350
4 neurons: loss: 0.5278 - accuracy: 0.7359 - val_loss: 0.5267 - val_accuracy: 0.7347
5 neurons : loss: 0.2872 - accuracy: 0.8833 - val_loss: 0.1370 - val_accuracy: 0.9747

100 epochs :

2 neurons : loss: 0.5232 - accuracy: 0.7297 - val_loss: 0.5243 - val_accuracy: 0.7284
3 neurons : loss: 6.2996e-05 - accuracy: 1.0000 - val_loss: 5.9360e-05 - val_accuracy: 1.0000 ---> overfitting epoch 23
4 neurons : loss: 1.5381e-05 - accuracy: 1.0000 - val_loss: 1.5039e-05 - val_accuracy: 1.0000 ---> overfitting epoch 18

2 neurons : loss: 0.0114 - accuracy: 0.9996 - val_loss: 0.0109 - val_accuracy: 0.9997
3 neurons : loss: 0.4826 - accuracy: 0.7478 - val_loss: 0.4814 - val_accuracy: 0.7485
4 neurons : loss: 0.1018 - accuracy: 0.9690 - val_loss: 0.0980 - val_accuracy: 0.9713


Differences car on regénère des données aléatoires entre chaque training
Résultats si on entraine toujours sur les mêmes données :

on fixe une base de données (fichier data.txt)
100 epochs :
1 neuron : loss: 0.5866 - accuracy: 0.6254 - val_loss: 0.5884 - val_accuracy: 0.6241 --> rien appris, même valeur depuis le début
2 neurons : loss: 0.0193 - accuracy: 0.9996 - val_loss: 0.0188 - val_accuracy: 0.9995
3 neurons : loss: 1.4735e-05 - accuracy: 1.0000 - val_loss: 1.4797e-05 - val_accuracy: 1.0000 --> overfitting à l'époque 27 : 
    loss: 0.0268 - accuracy: 0.9998 - val_loss: 0.0241 - val_accuracy: 0.9998
4 neurons : loss: 6.5338e-04 - accuracy: 1.0000 - val_loss: 6.0655e-04 - val_accuracy: 1.0000 --> overfitting à l'époque 74 : 
    loss: 0.0064 - accuracy: 0.9996 - val_loss: 0.0050 - val_accuracy: 0.9998
5 neurons : loss: 9.8681e-04 - accuracy: 1.0000 - val_loss: 9.4669e-04 - val_accuracy: 1.0000 --> overfitting à l'époque 30 : 
    loss: 0.0629 - accuracy: 0.9994 - val_loss: 0.0530 - val_accuracy: 0.9995

50 epochs : 
1 neuron :  loss: 0.6112 - accuracy: 0.6383 - val_loss: 0.6122 - val_accuracy: 0.6357 --> rien appris, même valeur depuis le début
2 neurons : loss: 0.4974 - accuracy: 0.7429 - val_loss: 0.5002 - val_accuracy: 0.7423
3 neurons : loss: 0.1317 - accuracy: 0.9663 - val_loss: 0.1311 - val_accuracy: 0.9673
4 neurons : loss: 0.0114 - accuracy: 0.9998 - val_loss: 0.0106 - val_accuracy: 0.9998
5 neurons: loss: 0.0094 - accuracy: 0.9999 - val_loss: 0.0087 - val_accuracy: 0.9999




b b b b b b a b b b b b b a b b b b b b a b b b b a b b b b
[[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0]]

[[-0.277715981 -0.0609863]]
[[-0.277715981 -0.0609863]]
[[-0.121935904 -0.071513474]]
[[-0.261825264 -0.0893834308]]
[[-0.169909298 -0.0886192247]]
[[-0.243698359 -0.0959995911]]
[[-0.192543894 -0.0943125784]]
[[0.933422506 -0.674193621]]
[[0.804650784 -0.128138065]]
[[0.713337362 -0.210288972]]
[[0.643343806 -0.304734826]]
[[0.592847526 -0.419017732]]
[[0.559230626 -0.527945518]]
[[0.535945296 -0.584827244]]
[[-0.0509327054 -0.970136881]]
[[-0.0731718242 -0.865513742]]
[[-0.0968646 -0.869320691]]
[[-0.121167511 -0.873101115]]
[[-0.146041363 -0.87654382]]
[[-0.17144914 -0.879531801]]
[[-0.197325334 -0.882010162]]
[[0.419803202 -1.6367085]]
[[0.190387115 -0.101187453]]
[[-0.0320316702 -0.14535746]]
[[-0.23271811 -0.159350067]]
[[-0.249439582 -0.145445377]]
[[0.92402786 -0.796385229]]
[[0.788541377 -0.118969671]]
[[0.69182688 -0.201235697]]
[[0.617096364 -0.295161188]]
[[0.562655747 -0.412141979]]

b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b
[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]

[[-0.277715981 -0.0609863]]
[[-0.277715981 -0.0609863]]
[[-0.121935904 -0.071513474]]
[[-0.261825264 -0.0893834308]]
[[-0.169909298 -0.0886192247]]
[[-0.243698359 -0.0959995911]]
[[-0.192543894 -0.0943125784]]
[[-0.231949121 -0.0977174193]]
[[-0.204008952 -0.0964590237]]
[[-0.225079939 -0.0981134623]]
[[-0.209960073 -0.0973289758]]
[[-0.221227616 -0.0981585309]]
[[-0.213085026 -0.0977021]]
[[-0.219111219 -0.0981268361]]
[[-0.214737087 -0.0978700519]]
[[-0.217961013 -0.0980906114]]
[[-0.215614453 -0.0979487821]]
[[-0.217339605 -0.098064445]]
[[-0.216081619 -0.0979870111]]
[[-0.217005119 -0.0980480835]]
[[-0.216330871 -0.098006025]]
[[-0.216825321 -0.0980383679]]
[[-0.216463983 -0.0980156735]]
[[-0.216728851 -0.0980329365]]
[[-0.2165353 -0.0980206877]]
[[-0.216677055 -0.0980298817]]
[[-0.216573402 -0.0980233327]]
[[-0.216649294 -0.0980282053]]
[[-0.216593802 -0.098024711]]
[[-0.216634393 -0.0980273]]
[[-0.216604665 -0.0980254]]

a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a
[[0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0]]

[[0.940785944 -0.464669168]]
[[0.940785944 -0.464669168]]
[[-0.149279535 -0.84660244]]
[[0.385762602 -1.54715455]]
[[0.758346498 -1.50694132]]
[[-0.049001798 -0.905963421]]
[[0.385538876 -1.55473256]]
[[0.730348468 -1.54166734]]
[[-0.0487197489 -0.928736866]]
[[0.378766388 -1.56768167]]
[[0.737681091 -1.55534124]]
[[-0.0494268052 -0.923735499]]
[[0.37824294 -1.56458211]]
[[0.738668263 -1.55371857]]
[[-0.049544692 -0.922938824]]
[[0.378334969 -1.56414604]]
[[0.738636 -1.55332494]]
[[-0.0495391414 -0.922943115]]
[[0.378360063 -1.56415367]]
[[0.73860538 -1.55330873]]
[[-0.0495356657 -0.922964871]]
[[0.378360689 -1.56416655]]
[[0.738602698 -1.55331707]]
[[-0.0495353788 -0.922967374]]
[[0.37836 -1.56416774]]
[[0.738603175 -1.55331862]]
[[-0.0495353788 -0.922966957]]
[[0.37836 -1.5641675]]
[[0.738603354 -1.5533185]]
[[-0.0495354 -0.922966838]]
[[0.378360033 -1.56416738]]