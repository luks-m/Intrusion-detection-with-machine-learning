# Réunion WhatsApp le 27/06 avec Omer Nguena Timo

### Objectifs

#### Débloquer Security Onion
Poser une question avec le problème sur les forums de Security Onion.
Essayer une nouvelle méthode pour l'installation.

#### Coder un automate - Finite-State Machine, FSM

Coder un automate à 4 états avec un langage d'entrée {a,b} et un langage de sortie {0,1}.  
Générer des séries aléatoires avec cet automate pour avoir des couples (entrée, sortie) puis les faire apprendre au réseau LSTM.  
Générer des traces d'entrées (abbabbbaa) et prédire les sorties (001011001).

Intégrer les log dans le réseau LSTM, et analyser les logs pour voir le nombre de sorties distinctes. Vérifier si 4 états = 4 sorties.

-> Explicabilité de l'IA

#### Ecriture d'un rapport sur LSTM et séries temporelles

Rapport d'explication des enjeux mais aussi de la réalisation.
- Comment fonctionne la descente de gradient d'un LSTM
- Possibilité de mettre du code avec des verbatim

### Réalisation

#### Implémentation de la Machine à états et génération des logs
En réalité on s'intéresse ici à un FST (Finite State Transducer) qui est un automate à états finis où chaque transition entre deux états est définie par une entrée mais aussi une sortie.
Ainsi en exécutant l'automate on obtient une série d'entrées/sorties.

J'ai donc implémenté une classe fsm qui représente un automate à états finis.
Il est possible de générer des entrées/sorties en exécutant la commande :
```bash
$ python3 log_generator.py
```

#### Traitement de ces logs avec un réseau LSTM
On généère un nombre de logs que l'on fixe, et de taille fixe car toutes les traces réseaux auront la même taille (entête). On peut ensuite réaliser des traitements sur les données pour notamment les encoder avec une encodeur one_hot (table de binaire). On modifie les outputs pour respecter ce format d'encodage one_hot.  
En sortie du réseau, on a donc pour chaque entrée, la probabilité que l'ouput correspondant soit un 1 ou un 0.  

On remarque facilement que le réseau peut vite être surentrainé. Il faut donc faire attention au nombre d'epochs.

Pour entrainer le réseau, il faut fixer une taille de log `LOG_SIZE` et un nombre de logs `NB_LOGS`.
Ensuite, il suffit d'exécuter la commande suivante :
```bash
$ python3 neural_network.py
```

Le meilleur apprentissage est sauvegardé dans le fichier `saved_model.h5`.
Il est donc possible de retester le model avec la commande :
```bash
$ python3 test.py
```

Il est possible d'accéder aux poids du réseau de neurones après apprentissage ainsi qu'à l'historique des loss et accuracy.
Le fichier `analyse_model.py` permet de réaliser des analyse sur le model et l'historique est stocké dans le fichier `history.tqt`.

#### Ecriture d'un rapport sur LSTM et séries temporelles
https://www.overleaf.com/project/62c84a015ab62c5af80df7fd  


