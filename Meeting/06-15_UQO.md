# Réunion UQO le 15/06 avec Omer Nguena Timo

### Objectifs pour le 20/06

#### Comprendre les algorithmes caché derrière l'apprentissage et la prédiction par LSTM
L'objectif est d'être capable de refaire l'algo et de l'expliquer avec ses mots, sans avoir à utiliser tensorflow ou PyTorch

```diff
- Backpropagation : comment ce fait réellement le changement des paramètres, poids et biais ?
+ exemple de code : https://www.tensorflow.org/guide/basic_training_loops  
```
```py
# Subtract the gradient scaled by the learning rate
  model.w.assign_sub(learning_rate * dw)
  model.b.assign_sub(learning_rate * db)
```

#### Prédiction par LSTM avec du texte en entrée  
Cela implique de reformater les données en entrées et de leur appliquer un pré-traitement
*Ex : Chatbot qui propose une liste de réponse en lien avec la question, suite de mots proposée par le clavier lors de l'écriture d'un message*

#### Refaire l'entrainement avec LSTM en mettant en entrée des réelles dataset de traces et log d'intrusion
Trouver les dataset, classer les fausses des vraies intrusions, puis entrainer le modèle
*Faire de l'entrainement sur des données de série temporelle pour détection d'intrusions*

#### Regarder ce qu'est Wazuh et selks
Petit résumé + voir si on peut les utiliser

- Lors du parcours des différentes articles : prendre du temps pour écrire ce qu'on a compris de l'article et pouvoir soulever des questions ou remarques.

### Réalisation

#### Qu'est-ce qu'un RNN et LSTM ? Quel est l'algorithme d'apprentissage ?
https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/?ref=lbp  

https://www.youtube.com/watch?v=EL439RMv3Xc&ab_channel=ThibaultNeveu  
https://www.youtube.com/watch?v=3xgYxrNyE54&ab_channel=ThibaultNeveu  
Vidéos d'explications sur RNN et LSTM

https://www.geeksforgeeks.org/lstm-derivation-of-back-propagation-through-time/?ref=lbp  
Explication de la backpropagation et algotithme de LSTM

https://agustinus.kristia.de/techblog/2016/08/12/lstm-backprop/  
Implémentation d'un algo d'apprentissage avec backpropagation en Python

**Résumé général**  

L'apprentissage de séries temporelles nécessite de garder en mémoire différents états des cellules lors du training.

Ainsi, les Réseaux de Neurones Récurrents permettent cela.

*Analogie avec les réseaux feedforward*  

On note X l'entrée dans le réseau et Y la sortie du réseau. On a alors X -> réseaux -> Y  
Lors de l'apprentissage dans un réseau de base, à chaque étapes on retourne une sortie en fonction de l'entrée uniquement. Cependant pour les séries temporelles, la sortie est prédit en fonction de l'entrée mais également de ce qu'il s'est passé à l'instant t-1.

Par exemple, si on envoie "Comment" dans le réseau, alors celui si va retourner "ça" qui est généralement le mot qui suit "comment" dans les phrases des français. Ensuite si on envoie "ça" dans le réseaux, alors il est difficile pour le réseau de comprendre ce qu'il faut prédire comme il ne connait pas le mot qui était en entrée avant "ça". C'est pourquoi il faut garder en mémoire les états des cellules, c'est ce que fait le réseau récurrent.

On a donc   Xt -> réseaux -> Yt  
                       |  
            Xt+1 -> réseaux -> Yt+1

Ainsi le réseau peut garder en mémoire l'état de la cellule à l'instant t-1 pour prédire la sortie à l'instant t.

*Limite du RNN, utilisation du LSTM*  

Cependant les RNN présentent une limite liée à la descente de gradient. En effet, lors de la backpropagation, le gradientest mulitplié à chaque étape par des valeurs très petites comprises entre 0 et 1 (cela est du à l'utilisation de la fonction d'activation `tanh`). Ainsi le gradient étant très petit, les valeurs des poids et biais ne sont plus modifiées et le réseau de neurone n'apprend plus.

C'est pour cela que l'on utilise le LSTM. Le LSTM permet de garder en mémoire *beaucoup de données à court terme*. En effet, chaque état des cellules est sauvergardé à l'instant t. A l'instant t+1, le LSTM utilise l'état précedent de la cellule mais peut modifier cet état. C'est-à-dire qu'il peut supprimer des informations, en modifier certaines et en ajouter des nouvelles. 

*Fonctionnement du LSTM*

Pour cela, le LSTM utilise 3 portes (gates). La **forget gate** qui définit quelle quantité d'information doit être oubliée, la **input gate** qui définit quelle quantité d'information doit être ajoutée et la **output gate** qui défninit le nouvel état de la cellule à partir des valeurs supprimées et ajoutées.

On peut shématiser une cellule du LSTM (Cf schéma papier). On connait les fonctions d'activation, on initialise les poids avec des valeurs nulles pour la première étape.

*Algorithme d'apprentissage*

On définit un nombre d'époch. A chaque époch, on train sur nos valeurs et on calcul l'erreur puis les différents gradient. Le gradient total est la somme de tous les gradients liés à chaque fonction d'activation (Cf Papier). Ensuite, on modifie la valeur des poids initiaux puis on relance l'apprentissage qui utilisera l'état de la cellule à l'instant t-1 et qui modifiera l'état en sortie avant qu'on recalcule le gradient et ainsi de suite jusqu'à la fin des itérations.

*Utilisation de TensorFlow*

Tensorflow propose ses propres méthodes d'apprentissage et sa propre boucle d'apprentissage avec backpropagation. Cependant, l'outil propose également de modifier certaines parties de la boucle (ou la boucle entière) ainsi que les méthodes d'apprentissage, c'est pourquoi il est très utile.

#### Wazuh
Wazuh est une plateforme d'analyse de sécurité qui permet de détecter les intrusions et les attaques.

#### Selks

#### Texte en entrée du réseau

*Preprocessing*

Le texte en entrée doit subir des modifications pour qu'il puisse correspondre à une entrée légitime du réseau.
En effet, le réseau attend un vecteur en entrée et non un texte.

Ainsi, on doit transformer le texte en vecteur. Pour cela, on suit les étapes suivantes:
- Conversion du texte en lower cases.
- Suppression des symbol et remplacement par *espace*.
- Suppression des symboles inutiles (chiffres, #, ...).
- Supression des XXX.
- Suppression des mots de fin de phrase.
- Tokenisation des textes en réparant chaque token distint.
- Redimension des vecteurs pour qu'ils aient tous la même taille.

*Implémentation*

Essai de la méthode sur une petite base de données de keras.
Possibilité d'exécuter la méthode avec la commande :
```bash
$ python3 text_classification.py
```
Les résultats sont affichés dans la console. Ils ne sont pas très précis étant donné la faible taille de la datasetet du nombre d'épochs. Cependant, si on augmente le nombre d'époch, comme la dataset reste petite, cela va créer un overfitting.

*Utilisation du modèle pour la détection d'intrusion*

Il est possible d'utiliser ce modèle pour détecter des intrusions. En effet, la classification permet de trier les traces et log d'un réseau et de les répartir en différentes catégories. L'objectif serait donc maintenant de trouver des données pour train un modèle, puis de détecter des intrusions. Et évaluer nos résultats.

#### LSTM pour l'apprentissage des intrusions

https://www.researchgate.net/publication/342754573_Long_Short-Term_Memory_LSTM_Deep_Learning_Method_for_Intrusion_Detection_in_Network_Security

Ce papier reprend une étude de classification de détection d'intrusion. Il correspond exactement à ce que nous voulons faire.

Voici les étapes à respecter : 
- Split la dataser en deux parties : train et test
- Pour chaque data : preprocess pour les convertir dans le bon format
- Sélection des variables
- Passage dans le réseau LSTM
- Prédiction de la sortie
- Evaluation des résultats

La base de donnée utilisée dans ce modèle est la base de donnée KDD99. Cependant, la version NSL-KDD est plus grande et plus complète. Ce sera donc celle là que nous utiliserons.

Il est possible de faire de la binary classification (0,1) : 0 for normal traffic and 1 for attack. Mais aussi de la multiclass classification qui permet de classer les données en plusieurs catégories.

