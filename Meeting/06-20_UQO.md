# Réunion UQO le 20/06 avec Omer Nguena Timo

### Objectifs pour le 23/06

#### Regarder LSTM et tâches de prédiction (et non classification)
Chercher à prédire les actions de l'attaquanten fonction des traces précédentes.

#### Embedding

### Réalisation

#### Embedding

https://medium.com/analytics-vidhya/understanding-embedding-layer-in-keras-bbe3ff1327ce#:~:text=Embedding%20layer%20is%20one%20of,word%20embeddings%20such%20as%20GloVe.

Lorsqu'on travaille avec des données textuelles dans les réseaux de neurones, il est nécessaire de les transformer en données numériques. Il est possible de réaliser cela en créant des variables (dummy features) et en créant une table contenant des 1 et des 0 en fonction des mots (dummy features) contenu dans le message. Cependant, pour un vocabulaire de grande taille, cela n'est pas réalisable de comparer et jouer avec des tables de grandes tailles. C'est pourquoi l'embedding est important. Il permet d'associer des vecteurs de taille fixe à chaque groupe de mots. Ainsi, à chaque mot est associé un encodage tel que l'encodage one-hot par exemple.

*Encodage one-hot*  
L'encodage one-hot est un encodage keras qui permet d'associer une vamleur numérique à chaque mot du vocabulaire définis. Il dépend donc du nombres de mots dans le vocabulaire.

L'encodage des valeurs textuelles permet ainsi de transormer chaque groupe de mots en un vecteur des encodages.  
Par exemple, si on a le groupe de mot **"Ceci est une intelligence artificielle"**, alors l'encodage sera de la forme `̀[12, 2, 1, 45, 34]` en supposant que le mot **Ceci** est encodé avec la valeur `12`, etc.

Afin d'obtenir des vecteurs que l'on peut passer en entrée de notre réseau de neurones, il est alors nécessaire de tous leur donner le même taille. On ajoute donc du padding aux vecteurs les plus courts de telle sorte à ce que tous les vecteurs aient la longueur du vecteur le plus long.

La couche embedding permet alors de transormer les mots en vecteurs denses de taille fixe. Ces vecteurs contiennent des valeurs réelles (plutot que des 1 et 0 pour une analyse basique avec get_dummies). Ainsi un groupe de mots de taille `[1, 2, 3]` sera transformé en un vecteur de taille `[3, 3, 3]` et chacune de ses composantes sera un vecteur dense représentant un mot.

La couche embedding est ainsi similaire à une table de comparaison, les mots y sont les clés et les vecteurs dense les valeurs.

Dans keras, la couche Embedding se définit avec 3 paramètres
- input_dim : le nombre de mots dans le vocabulaire
- output_dim : la taille du vecteur dense
- input_length : la taille du vecteur d'entrée

Ainsi l'utilisation de Embedding avec des données textuelles se fait comme suit :
- On encode les groupes de mots avec un encodage one-hot de keras par exemple
- On complète les vecteurs les plus courts avec du padding si nécessaire
- On choisi la taille de notre vocabulaire, ainsi que la taille des vecteurs en sortie de la couche Embedding (quelle taille de vecteur pour désigner un mot)
- On passe notre entrée dans le réseau.

Les poids du réseau, une fois entrainés, correspondent ainsi à l'encodage des mots du vocabulaire. Il est donc facile pour le réseau, lorsqu'on lui passe un nouveau groupe de mots en entrée, de pouvoir analyser la sortie correspondante en analysant la composition du message. En effet, la couche embedding prend le message (le vecteur des vecteurs des mots du message), et renvoie un tensor des vecteurs des poids associés aux mots en entrées. Ainsi ce tensor est prêt à être envoyé à la couche LSTM et encode bien les mots présents dans le message.

Par exemple, si on a les poids de la couche embedding entrainés pour prédire des nombres de 0 à 3, avec des tailels de vecteurs (output_dim) de 4. Alors on a cela : 
```python
   [[-0.04333381, -0.02326865, -0.00812379,  0.02167496],
    [ 0.04502351,  0.00151128,  0.01764284, -0.0089057 ],
    [-0.04007018,  0.02874336,  0.02772436,  0.00842067],
    [ 0.00512743,  0.03695237, -0.02774147, -0.03748262]]
```

La ligne 1 encodant le chiffre 0, la 2 le chiffre 1, etc. Ainsi si on envoie le groupe de mots [1, 2], alors la couche embedding renvoie le tensor :
```python
   [[0.04502351,  0.00151128,  0.01764284, -0.0089057 ],
    [-0.04007018,  0.02874336,  0.02772436,  0.00842067]]
```
La couche LSTM va donc travailler avec ce tensor et le réseau connait à quel mot ces valeurs correspondent.

La commande `$ python3 embedding.py` permet de tester la couche Embedding et d'afficher les explications.

#### LSTM pour la prédiction

https://ishwargautam.blogspot.com/2021/07/next-word-prediction-using-lstm.html
Papier sur la prédiction de mots à partir de mots déjà écris utilisant les réseaux LSTM.

*Implémentation*

Pour implémenter la prédiction de mots par un réseau LSTM, on prend une base de donnée. Nous avons ici pris *Le tour du monde en 80 jours*. Afin de limiter la durée de l'apprentissage, nous avons intégré la gestion du nombre de lignes. Ainsi, nous limitons les données d'apprentissage à un ceratin nombre de ligne. Ensuite, on préprocess les données, c'est à dire qu'on enlève les caractères non utiles, la ponctuation, etc... Puis on convertit les mots en valeurs numériques. Enfin, on utilise une couche d'embedding afin de représenter les mots par des vecteurs dense tous de même taille.

La couche LSTM peut alors être utilisée pour prédire des mots à partir des mots précédents.

Le paramètre WINDOW_SIZE permet de définir la taille de la fenêtre de prédiction. C'est à dire le nombre de mots nécéssaires pour prédire le mot suivant.

Le model a été entrainé sur tous le livre *Le tour du monde en 80 jours* avec un réseau LSTM de taille de 100. Puis il a été sauvegardé dans les fichiers `next_word.h5`et `token.pkl`. Ainsi il est possible de tester notre modèle en excutant la commande `$ python3 next_word.py` à partir du répertoire **word_prediction**.

